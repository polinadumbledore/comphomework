{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f648baa3",
   "metadata": {},
   "source": [
    "Датасет IMDB Dataset Split из kaggle - https://www.kaggle.com/datasets/ducanger/imdb-dataset?resource=download&select=test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df4264",
   "metadata": {},
   "source": [
    "Я невнимательно прочитала задание и заметила это, когда было поздно... вместо того, чтобы смотреть, каких слов в отзыве больше - из положительного списка или из отрицательного, сделала логистической регрессией. Но в конце все равно есть деление на эти списки, я просто не успела доделать это тем способом, которым надо было"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d774fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "842df2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3923c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6107ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a70ae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(inputfile):\n",
    "    inputtext = []\n",
    "    for text in inputfile:\n",
    "        word_list = nltk.word_tokenize(text)\n",
    "        lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "        inputtext.append(lemmatized_output)\n",
    "    return inputtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87065691",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\") \n",
    "test = pd.read_csv(\"test.csv\") # прочитала файлы\n",
    "lemma_test = lemmatize(test['review'])\n",
    "lemma_train = lemmatize(train['review']) #лемматизировала файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8592b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(binary=True, max_features=10000) \n",
    "cvec.fit(lemma_train)\n",
    "\n",
    "X_train = cvec.transform(lemma_train).astype(int)\n",
    "X_test = cvec.transform(lemma_test).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a4d197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = cvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5ecfb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# использую логиситическую регрессию\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "y_train = [bool(i) for i in train[\"sentiment\"]] # завожу переменную для ответов, где True = полож,, False = отриц. отзыв\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cf7fbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train) # завожу модель, обучаю\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05181315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score # загружаю метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be43cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test) # предсказываю ответ\n",
    "y_test = [bool(i) for i in test[\"sentiment\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9107e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = y_pred.tolist() # делаю из array список\n",
    "true_pred_list = [] \n",
    "for prediction in pred_list:\n",
    "    if prediction == True:\n",
    "        true_pred_list.append(1)\n",
    "    if prediction == False:\n",
    "        true_pred_list.append(0) # меняю True и False на 1 и 0 соответственно\n",
    "result = test[[\"review\"]].copy()\n",
    "result[\"sentiment\"] = true_pred_list \n",
    "result.to_csv(\"resultnew.csv\", index=False) # создаю csv табличку с результатами\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62dd6231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8711"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7820568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "df = pd.read_csv(\"train.csv\", sep=\",\")\n",
    "df =df.head(100000)\n",
    "\n",
    "dictrev = pd.Series(df.sentiment.values,index=df.review).to_dict() #сделала из csv dict\n",
    "posrev = []\n",
    "negrev = []\n",
    "for key, value in dictrev.items(): #добавляю все положительные отзывы в один список, все отрицательные - в другой\n",
    "    if value == 1:\n",
    "        posrev.append(key)\n",
    "    if value == 0:\n",
    "        negrev.append(key)\n",
    "posrev = \" \".join(posrev)\n",
    "negrev = \" \".join(negrev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94cdf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_posrev = nltk.word_tokenize(posrev)\n",
    "tokenized_negrev = nltk.word_tokenize(negrev) #токенизирую  отзывы\n",
    "onlypos = []\n",
    "onlyneg = []\n",
    "for word in tokenized_posrev: # делаю списки, где только те слова, которых нет в другом списке\n",
    "    if word not in tokenized_negrev:\n",
    "        onlypos.append(word)\n",
    "for word in tokenized_negrev:\n",
    "    if word not in tokenized_posrev:\n",
    "        onlyneg.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0192f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "posunigrams = Counter(onlypos)\n",
    "negunigrams = Counter(onlyneg)\n",
    "print(posunigrams)\n",
    "print(negunigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eec376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
